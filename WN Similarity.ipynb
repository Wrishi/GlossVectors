{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating gloss vector from nltk corpus (brown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment to check out on smaller scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.sparse import bsr_matrix\n",
    "import nltk.corpus\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Position surrounding word\n",
    "num_pos = 4\n",
    "keyword = \"serve\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-400dc97bd494>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mword_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbrown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfiltered_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_list\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-400dc97bd494>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mword_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbrown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfiltered_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_list\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Work/nltk/nltk/corpus/reader/wordlist.py\u001b[0m in \u001b[0;36mwords\u001b[0;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \"\"\"\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_lines_startswith\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         return [line for line in line_tokenize(self.raw(fileids))\n\u001b[0m\u001b[1;32m     23\u001b[0m                 if not line.startswith(ignore_lines_startswith)]\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Work/nltk/nltk/corpus/reader/wordlist.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_lines_startswith\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         return [line for line in line_tokenize(self.raw(fileids))\n\u001b[0;32m---> 23\u001b[0;31m                 if not line.startswith(ignore_lines_startswith)]\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "word_list = nltk.corpus.brown.words()\n",
    "filtered_words = [word for word in word_list if word not in nltk.corpus.stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(filtered_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [i for i, x in enumerate(filtered_words) if x == keyword]\n",
    "print(len(indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "just_words = tokenizer.tokenize(\" \".join(word_list))\n",
    "just_words = [x.lower() for x in just_words]\n",
    "unique_words = list(set(just_words))\n",
    "print(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_words_cp = filtered_words.copy()\n",
    "print(filtered_words_cp[360:370])\n",
    "print(filtered_words_cp[43340:43350])\n",
    "index = filtered_words_cp.index(keyword)\n",
    "print(index)\n",
    "filtered_words_cp.pop(index)\n",
    "index = filtered_words_cp.index(keyword)\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gloss_vector = np.zeros(len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [i for i, x in enumerate(filtered_words) if x == keyword]\n",
    "words_around = []\n",
    "for i in indices:\n",
    "    words_around.extend(filtered_words[i - num_pos : i])\n",
    "    words_around.extend(filtered_words[i + 1 : i + num_pos + 1])\n",
    "\n",
    "print(words_around)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in words_around:\n",
    "    if word in unique_words:\n",
    "        index = unique_words.index(word)\n",
    "        gloss_vector[index] += 1\n",
    "        \n",
    "print(gloss_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [i for i, x in enumerate(gloss_vector) if x > 0]\n",
    "for i in indices:\n",
    "    print(unique_words[i], \": \", gloss_vector[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gloss_vector[unique_words.index(\"tennis\")])\n",
    "print(gloss_vector[unique_words.index(\"purpose\")])\n",
    "print(gloss_vector[unique_words.index(\"bird\")])\n",
    "print(gloss_vector[unique_words.index(\"dinner\")])\n",
    "print(gloss_vector[unique_words.index(\"lunch\")])\n",
    "print(gloss_vector[unique_words.index(\"breakfast\")])\n",
    "print(gloss_vector[unique_words.index(\"food\")])\n",
    "print(gloss_vector[unique_words.index(\"waiter\")])\n",
    "print(gloss_vector[unique_words.index(\"restaurant\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "c = 0\n",
    "for ss in wn.all_synsets():\n",
    "    # ss = wn.synset(\"dog\")\n",
    "    print(ss)\n",
    "    #print(ss.attributes())\n",
    "    #print(ss.also_sees())\n",
    "    print(ss.causes())\n",
    "    #print(ss.closure())\n",
    "    #print(ss.common_hypernyms())\n",
    "    print(ss.definition())\n",
    "    print(ss.entailments())\n",
    "    print(ss.examples())\n",
    "    print(ss.frame_ids())\n",
    "    print(ss.hypernym_distances())\n",
    "    print(ss.hypernym_paths())\n",
    "    print(ss.hypernyms())\n",
    "    print(ss.hyponyms())\n",
    "    print(ss.instance_hypernyms())\n",
    "    print(ss.instance_hyponyms())\n",
    "    #print(ss.jcn_similarity())\n",
    "    #print(ss.lch_similarity())\n",
    "    print(ss.lemma_names())\n",
    "    print(ss.lemmas())\n",
    "    print(ss.lexname())\n",
    "    #print(ss.lin_similarity())\n",
    "    #print(ss.lowest_common_hypernyms())\n",
    "    print(ss.max_depth())\n",
    "    print(ss.member_holonyms())\n",
    "    print(ss.member_meronyms())\n",
    "    print(ss.min_depth())\n",
    "    print(ss.name())\n",
    "    print(ss.offset())\n",
    "    print(ss.part_holonyms())\n",
    "    print(ss.part_meronyms())\n",
    "    #print(ss.path_similarity())\n",
    "    print(ss.pos())\n",
    "    print(ss.region_domains())\n",
    "    #print(ss.res_similarity())\n",
    "    print(ss.root_hypernyms())\n",
    "    #print(ss.shortest_path_distance())\n",
    "    print(ss.similar_tos())\n",
    "    print(ss.substance_holonyms())\n",
    "    print(ss.substance_meronyms())\n",
    "    print(ss.topic_domains())\n",
    "    #print(ss.tree())\n",
    "    print(ss.unicode_repr())\n",
    "    print(ss.usage_domains())\n",
    "    print(ss.verb_groups())\n",
    "    #print(ss.wup_similarity())\n",
    "    print(\"*****************************************************\")\n",
    "    c += 1\n",
    "    if c > 9:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "for ss in wn.all_synsets():\n",
    "    c += 1\n",
    "    \n",
    "print(c)\n",
    "num_synsets = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "for w in wn.words():\n",
    "    c += 1\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main implemantation starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# Creating corpus\n",
    "wn_corpus = \"\"\n",
    "covered = []\n",
    "for ss in wn.all_synsets():\n",
    "    w = ss.name().split(\".\")[0]\n",
    "    if w not in covered:\n",
    "        syns = wn.synsets(w)\n",
    "        for s in syns:\n",
    "            wn_corpus += s.definition() + \". \" + \". \".join(s.examples()) + \". \"\n",
    "        \n",
    "        covered.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86555"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(covered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(usually followed by `to') having the necessary means or skill or know-how or authority to do someth\n",
      "12504224\n"
     ]
    }
   ],
   "source": [
    "print(wn_corpus[0:100])\n",
    "print(len(wn_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus to array\n",
    "wn_words = re.sub(\"[^\\w]\", \" \",  wn_corpus).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['followed', 'by', 'to', 'having', 'the', 'necessary', 'means', 'or', 'skill']\n",
      "2043791\n"
     ]
    }
   ],
   "source": [
    "print(wn_words[1:10])\n",
    "print(len(wn_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Stopwords\n",
    "wn_filtered_words = [word for word in wn_words if word not in nltk.corpus.stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['followed', 'necessary', 'means', 'skill', 'know', 'authority', 'something', 'able', 'swim']\n",
      "1215318\n"
     ]
    }
   ],
   "source": [
    "print(wn_filtered_words[1:10])\n",
    "print(len(wn_filtered_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing low frequency words\n",
    "wn_dict = {}\n",
    "for w in wn_filtered_words:\n",
    "    word = w.lower()\n",
    "    if word not in wn_dict.keys():\n",
    "        wn_dict[word] = 0\n",
    "    \n",
    "    wn_dict[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55343"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wn_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3012"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn_dict[\"usually\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting word frequencies\n",
    "plt.bar(range(len(wn_dict)), list(wn_dict.values()), align='center')\n",
    "plt.xticks(range(len(wn_dict)), list(wn_dict.keys()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn_dict_copy = wn_dict.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29441\n",
      "25902\n"
     ]
    }
   ],
   "source": [
    "# Number of words for which frequency is less than num are chucked out\n",
    "# [Selected 4 since it give the size that matches with paper]\n",
    "num = 4\n",
    "c = 0\n",
    "for w, n in wn_dict_copy.items():\n",
    "    if n < num:\n",
    "        del wn_dict[w]\n",
    "        c += 1\n",
    "\n",
    "print(c)\n",
    "print(len(wn_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn_unique_words = list(wn_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25902"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wn_unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn_gloss_matrix = bsr_matrix((len(wn_unique_words), len(wn_unique_words)), dtype=np.int8).toarray()\n",
    "r = 0\n",
    "for w in wn_unique_words:\n",
    "    wn_words_around = []\n",
    "    indices = [i for i, x in enumerate(wn_filtered_words) if x == w]\n",
    "    for i in indices:\n",
    "        wn_words_around.extend(wn_filtered_words[i - num_pos : i])\n",
    "        wn_words_around.extend(wn_filtered_words[i + 1 : i + num_pos + 1])\n",
    "    \n",
    "    for word in wn_words_around:\n",
    "        if word in wn_unique_words:\n",
    "            index = wn_unique_words.index(word)\n",
    "            wn_gloss_matrix[r][index] += 1\n",
    "    \n",
    "    r += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wn.synsets(\"dog\"))\n",
    "for s in wn.synsets(\"dog\"):\n",
    "    print(s.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('gloss_matrix.v01.npz', wn_gloss_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[42 49  8 ...  0  0  0]\n",
      "[48 12  3 ...  0  0  0]\n",
      "[7 2 4 ... 0 0 0]\n",
      "[14  1  9 ...  0  0  2]\n",
      "[2 2 2 ... 0 0 0]\n",
      "[17  2  2 ...  0  0  0]\n",
      "[5 0 3 ... 0 0 0]\n",
      "[106   8  18 ...   0   0   0]\n",
      "[1 1 0 ... 0 0 0]\n",
      "[0 0 0 ... 0 0 0]\n",
      "[7 1 0 ... 0 0 0]\n",
      "[5 0 2 ... 0 0 0]\n",
      "[3 0 0 ... 0 0 0]\n",
      "[3 0 0 ... 0 0 0]\n",
      "[11  0  0 ...  0  0  0]\n",
      "[17  0  2 ...  0  0  0]\n",
      "[0 1 0 ... 0 0 0]\n",
      "[2 0 0 ... 0 0 0]\n",
      "[0 0 0 ... 0 0 0]\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(wn_gloss_matrix[0])\n",
    "print(wn_gloss_matrix[1])\n",
    "print(wn_gloss_matrix[2])\n",
    "print(wn_gloss_matrix[3])\n",
    "print(wn_gloss_matrix[4])\n",
    "print(wn_gloss_matrix[5])\n",
    "print(wn_gloss_matrix[6])\n",
    "print(wn_gloss_matrix[7])\n",
    "print(wn_gloss_matrix[8])\n",
    "print(wn_gloss_matrix[9])\n",
    "print(wn_gloss_matrix[10])\n",
    "print(wn_gloss_matrix[11])\n",
    "print(wn_gloss_matrix[12])\n",
    "print(wn_gloss_matrix[13])\n",
    "print(wn_gloss_matrix[14])\n",
    "print(wn_gloss_matrix[15])\n",
    "print(wn_gloss_matrix[16])\n",
    "print(wn_gloss_matrix[17])\n",
    "print(wn_gloss_matrix[18])\n",
    "print(wn_gloss_matrix[19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  42   49    8   15    2   17    5  106    1    0    7    5    3    3\n",
      "   11   17    0    2    0    0   11   14    1    8    0   27   24    4\n",
      "    7    0    0    6    5    9    5   13    9    0   10   83    7    0\n",
      "   10    5   14    0   49    0    6    5   29   39    8    3    4   35\n",
      "    6    2    5    0    5    0    2   10    1    1   11    0    0    5\n",
      "   33    0   18    1    2   14    1    7    3   31    8    3   13    2\n",
      "    0    9    2    3    2   16 -107    6   17    1    0    7    3   39\n",
      "    4    1    0    1   45    0   15    2    5    0    0    0   13    0\n",
      "    4    0   16    0    2    0    2    0    2    0   19   15    1    6\n",
      "    1   11    4    5    0    7    0   33    3   11    1    1   11    0\n",
      "    1    9    3    0    2    1    0    0    1    4    0    0    4    0\n",
      "    1    0    0    0    0    7    0    0   11   16    3    2    7    0\n",
      "    0   20    8    3    0    4    0    0   14    1    2    0    1    0\n",
      "    4    2    5    1    6    0    7    1    0    0   22    8    1    1\n",
      "    1    2    2    1]\n"
     ]
    }
   ],
   "source": [
    "print(wn_gloss_matrix[0][0:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "serve = wn_gloss_matrix[wn_unique_words.index(\"serve\")]\n",
    "food = wn_gloss_matrix[wn_unique_words.index(\"food\")]\n",
    "tennis = wn_gloss_matrix[wn_unique_words.index(\"tennis\")]\n",
    "spoon = wn_gloss_matrix[wn_unique_words.index(\"spoon\")]\n",
    "player = wn_gloss_matrix[wn_unique_words.index(\"player\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_serve = np.where(np.array(serve) > 0)\n",
    "in_food = np.where(np.array(food) > 0)\n",
    "in_tennis = np.where(np.array(tennis) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_int_serve_food = np.intersect1d(in_serve, in_food)\n",
    "in_int_tennis_food = np.intersect1d(in_tennis, in_food)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['usually', 'means', 'something', 'able', 'computer', 'well', 'children', 'young', 'capacity', 'two', 'body', 'man', 'served', 'army', 'without', 'power', 'like', 'hot', 'away', 'surface', 'upper', 'small', 'eye', 'action', 'open', 'relating', 'time', 'death', 'old', 'may', 'little', 'almost', 'way', 'accident', 'wine', 'anger', 'religious', 'person', 'act', 'unit', 'parts', 'event', 'device', 'foot', 'period', 'bad', 'water', 'food', 'several', 'immediately', 'one', 'game', 'place', 'freedom', 'got', 'hospital', 'made', 'put', 'meat', 'animal', 'music', 'first', 'title', 'someone', 'tennis', 'opening', 'gave', 'class', 'make', 'back', 'turn', 'left', 'right', 'group', 'form', 'certain', 'could', 'often', 'another', 'medium', 'create', 'data', 'disk', 'carry', 'function', 'cause', 'diet', 'male', 'animals', 'blood', 'hand', 'machine', 'sold', 'plants', 'play', 'relation', 'control', 'limited', 'expressing', 'direct', 'guide', 'take', 'chief', 'us', 'equipment', 'trained', 'women', 'course', 'disease', 'child', 'people', 'state', 'pay', 'support', 'war', 'oneself', 'feeling', 'found', 'related', 'common', 'material', 'substance', 'etc', 'environment', 'country', 'light', 'heat', 'ready', 'system', 'large', 'compound', 'serving', 'solid', 'table', 'much', 'practice', 'e', 'g', 'excessively', 'eat', 'consume', 'specific', 'particular', 'purpose', 'building', 'composed', 'mass', 'formed', 'intended', 'personal', 'purposes', 'less', 'name', 'woman', 'meal', 'regularly', 'property', 'land', 'movement', 'room', 'effort', 'circumstances', 'illegally', 'relatively', 'men', 'normal', 'head', 'soldiers', 'organization', 'file', 'social', 'restaurant', 'quality', 'cooked', 'set', 'handle', 'requirements', 'adequate', 'writing', 'meet', 'reach', 'mountains', 'needs', 'provide', 'hold', 'service', 'whose', 'shop', 'fill', 'military', 'information', 'sick', 'teeth', 'method', 'school', 'various', 'medical', 'reaction', 'general', 'plural', 'basis', 'either', 'gods', 'cannot', 'girl', 'member', 'designed', 'female', 'source', 'interesting', 'spoon', 'order', 'stores', 'agent', 'area', 'amounts', 'upon', 'goes', 'looking', 'government', 'dish', 'mouth', 'contains', 'modified', 'necessarily', 'waiting', 'colony', 'sea', 'ship', 'personnel', 'patients', 'cells', 'insect', 'drinks', 'turkey', 'carried', 'advance', 'troops', 'correspond', 'branches', 'kitchen', 'assembled', 'games', 'type', 'promote', 'poisoning', 'developed', 'larger', 'benefit', 'since', 'vessel', 'message', 'officer', 'parents', 'double', 'abnormal', 'stir', 'liking', 'serves', 'container', 'later', 'bodies', 'cooling', 'region', 'input', 'whales', 'partly', 'structure', 'lady', 'six', 'shell', 'pointed', 'mythology', 'sterile', 'transmission', 'cook', 'preferred', 'bound', 'domestic', 'meals', 'studies', 'professional', 'gills', 'jelly', 'household', 'homeless', 'bee', 'secreted', 'nutrition', 'ladle', 'jaws', 'vegetable', 'mollusks', 'supplement', 'wartime']\n",
      "['something', 'last', 'car', 'children', 'two', 'feet', 'man', 'without', 'lacking', 'like', 'surface', 'especially', 'open', 'time', 'point', 'love', 'cut', 'process', 'person', 'act', 'dead', 'unit', 'holding', 'device', 'days', 'food', 'several', 'immediately', 'others', 'one', 'game', 'got', 'shot', 'resembling', 'made', 'piece', 'animal', 'first', 'someone', 'cuts', 'bother', 'good', 'tennis', 'division', 'give', 'gave', 'many', 'make', 'back', 'left', 'along', 'line', 'discharge', 'form', 'often', 'major', 'performance', 'sugar', 'strength', 'grass', 'hand', 'full', 'play', 'control', 'take', 'camp', 'women', 'manner', 'around', 'similar', 'experience', 'events', 'longer', 'people', 'living', 'keep', 'oneself', 'great', 'mother', 'use', 'size', 'energy', 'etc', 'country', 'light', 'soft', 'hard', 'large', 'serving', 'fabric', 'table', 'excess', 'e', 'g', 'money', 'boy', 'characterized', 'woman', 'income', 'property', 'supply', 'party', 'easy', 'house', 'even', 'speech', 'top', 'men', 'sounds', 'head', 'organization', 'fresh', 'set', 'hat', 'acceptable', 'makes', 'clean', 'high', 'needed', 'service', 'second', 'break', 'level', 'ground', 'wood', 'four', 'important', 'pool', 'club', 'area', 'exhausted', 'sport', 'seasoned', 'return', 'consisting', 'ship', 'serve', 'dinner', 'carrying', 'advance', 'advantage', 'measuring', 'officer', 'horror', 'hunted', 'competition', 'caught', 'flood', 'neck', 'sets', 'equipped', 'bridge', 'chain', 'dividing', 'passes', 'swimming', 'annual', 'outdoor', 'crane', 'cheap', 'international', 'chops', 'walled', '1880']\n"
     ]
    }
   ],
   "source": [
    "a1 = []\n",
    "for i in in_int_serve_food:\n",
    "    a1.append(wn_unique_words[i])\n",
    "\n",
    "a2 = []\n",
    "for i in in_int_tennis_food:\n",
    "    a2.append(wn_unique_words[i])\n",
    "    \n",
    "print(a1)\n",
    "print(a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89.7971720236239"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "math.acos(np.dot(serve, food)/(np.linalg.norm(serve)* np.linalg.norm(food))) * 180/math.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90.62270722408209"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.acos(np.dot(serve, tennis)/(np.linalg.norm(serve)* np.linalg.norm(tennis))) * 180/math.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90.48953209153744"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.acos(np.dot(spoon, food)/(np.linalg.norm(spoon)* np.linalg.norm(food))) * 180/math.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91.51636275815346"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.acos(np.dot(spoon, tennis)/(np.linalg.norm(spoon)* np.linalg.norm(tennis))) * 180/math.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90.07699157279768"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.acos(np.dot(player, food)/(np.linalg.norm(player)* np.linalg.norm(food))) * 180/math.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90.60762086020966"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.acos(np.dot(player, tennis)/(np.linalg.norm(player)* np.linalg.norm(tennis))) * 180/math.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('wn_words.npz', wn_unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn_dict_copy[\"jewel\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
